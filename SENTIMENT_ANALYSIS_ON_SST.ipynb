{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SENTIMENT_ANALYSIS_ON_SST",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avinashbabu7/SENTIMENTAL_ANALYSIS/blob/main/SENTIMENT_ANALYSIS_ON_SST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4t0C4W2EZ8T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b0a776a-beb8-4e6c-8891-03a878268e38"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOGgs8l6D_r3"
      },
      "source": [
        "Importing libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKauaxpxJa94"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "lr=LogisticRegression()\n",
        "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "de_gi= DecisionTreeClassifier(criterion = \"gini\", random_state = 100,\n",
        "                               max_depth=2, min_samples_leaf=5)\n",
        "km=KMeans(n_clusters=2)\n",
        "rf = RandomForestClassifier()\n",
        "sv=SVC(kernel='poly')\n",
        "xgb_1=XGBClassifier()\n",
        "gb_1=GradientBoostingClassifier()\n",
        "gnb_1=GaussianNB()\n",
        "smx=SGDClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx3NzUviEadg"
      },
      "source": [
        "Loading data set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nf0UjYexJSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "564e0ae5-2ef4-4bf4-d50e-0394361b17b3"
      },
      "source": [
        "\n",
        "!pip install pytreebank \n",
        "import pytreebank\n",
        "sst = pytreebank.load_sst()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytreebank in /usr/local/lib/python3.7/dist-packages (0.2.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWw17sbFEkOA"
      },
      "source": [
        "obtaining  data-set as sentences\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBIvBiNwEoMz",
        "outputId": "0260d607-b848-4ca9-b12a-17527f04cba1"
      },
      "source": [
        "import pandas as pd\n",
        "data_train = [\n",
        "                (\n",
        "                    tree.to_lines()[0],\n",
        "                    tree.label\n",
        "                )\n",
        "                for tree in sst['train']\n",
        "            ]\n",
        "data_test = [(\n",
        "  tree.to_lines()[0],\n",
        "  tree.label\n",
        ")\n",
        "for tree in sst['test']\n",
        "]\n",
        "x_train=pd.DataFrame(data_train,columns=['sentence','label'])\n",
        "x_test=pd.DataFrame(data_test,columns=['sentence','label'])\n",
        "x_train_sent=x_train[['sentence']]\n",
        "print(x_train_sent)\n",
        "y_train_ind=x_train['label']\n",
        "print(y_train_ind)\n",
        "x_test_sent=x_test[['sentence']]\n",
        "y_test_ind=x_test['label']\n",
        "print(x_test_sent)\n",
        "print(y_test_ind)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                               sentence\n",
            "0     The Rock is destined to be the 21st Century 's...\n",
            "1     The gorgeously elaborate continuation of `` Th...\n",
            "2     Singer/composer Bryan Adams contributes a slew...\n",
            "3     You 'd think by now America would have had eno...\n",
            "4                  Yet the act is still charming here .\n",
            "...                                                 ...\n",
            "8539                                    A real snooze .\n",
            "8540                                     No surprises .\n",
            "8541  We 've seen the hippie-turned-yuppie plot befo...\n",
            "8542  Her fans walked out muttering words like `` ho...\n",
            "8543                                In this case zero .\n",
            "\n",
            "[8544 rows x 1 columns]\n",
            "0       3\n",
            "1       4\n",
            "2       3\n",
            "3       2\n",
            "4       3\n",
            "       ..\n",
            "8539    0\n",
            "8540    1\n",
            "8541    3\n",
            "8542    0\n",
            "8543    1\n",
            "Name: label, Length: 8544, dtype: int64\n",
            "                                               sentence\n",
            "0                        Effective but too-tepid biopic\n",
            "1     If you sometimes like to go to the movies to h...\n",
            "2     Emerges as something rare , an issue movie tha...\n",
            "3     The film provides some great insight into the ...\n",
            "4     Offers that rare combination of entertainment ...\n",
            "...                                                 ...\n",
            "2205                   An imaginative comedy/thriller .\n",
            "2206                      ( A ) rare , beautiful film .\n",
            "2207                 ( An ) hilarious romantic comedy .\n",
            "2208                Never ( sinks ) into exploitation .\n",
            "2209                        ( U ) nrelentingly stupid .\n",
            "\n",
            "[2210 rows x 1 columns]\n",
            "0       2\n",
            "1       3\n",
            "2       4\n",
            "3       2\n",
            "4       4\n",
            "       ..\n",
            "2205    3\n",
            "2206    4\n",
            "2207    4\n",
            "2208    3\n",
            "2209    0\n",
            "Name: label, Length: 2210, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHg7cncPF06K"
      },
      "source": [
        "preprocessing data set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9ivDlupKG1P",
        "outputId": "0b69518d-4790-4ce5-cc92-cc85ee479a05"
      },
      "source": [
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer(nb_words=2500, lower=True,split=' ')\n",
        "tokenizer.fit_on_texts(x_train_sent['sentence'])\n",
        "tokenizer.fit_on_texts(x_test_sent['sentence'])\n",
        "# print(len(tokenizer.word_index))\n",
        "X = tokenizer.texts_to_sequences(list(x_train_sent['sentence']))\n",
        "x=tokenizer.texts_to_sequences(list(x_test_sent['sentence']))\n",
        "x=pad_sequences(x)\n",
        "X = pad_sequences(X)\n",
        "#print(x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/text.py:180: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwTIghyrFAAE"
      },
      "source": [
        "Vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jXnNV4bKqgx"
      },
      "source": [
        "import pickle\n",
        "# embeding_matrix=np.zeros((len(),300))\n",
        "vocabulary=[]\n",
        "for token,index in tokenizer.word_index.items():\n",
        "  vocabulary.insert(index,token) \n",
        "#pickle.dump(vocabulary,open('/content/drive/My Drive/vocabulary.pickle', 'wb'))\n",
        "# for wor in vocabulary:\n",
        "# len(vocabulary)\n",
        "# len(tokenizer.word_index.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3Uxa_sgFLy9"
      },
      "source": [
        "loading glove embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn5RqaezKtJi"
      },
      "source": [
        "glove_embed_sst=pickle.load(open('/content/drive/My Drive/Glove_embed_sst.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvhpHOuTFHJP"
      },
      "source": [
        "embedding_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaSxfLpSKvXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af12e7d7-8977-4d8c-c7df-dc89fd2b4763"
      },
      "source": [
        "embedding_matrix=np.zeros((len(vocabulary),300))\n",
        "for word,vec in glove_embed_sst.items():\n",
        "  ind=vocabulary.index(word)\n",
        "  embedding_matrix[ind]=vec\n",
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17087, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tobQBasZFPjo"
      },
      "source": [
        "LSTM/Bi_LSTM for random embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMrgKH64KysB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf47cd0-4aa6-46de-a504-f8978bed701a"
      },
      "source": [
        "#this is for random embedded sentences using lstm and bi-lstm\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding,Bidirectional\n",
        "from keras.layers import Dense\n",
        "lstm_out = 300\n",
        "batch_size = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(vocabulary), 300,input_length = X.shape[1]))\n",
        "model.add(Masking(mask_value=0))\n",
        "\n",
        "model.add(Bidirectional(LSTM(lstm_out, recurrent_dropout= 0.2, dropout= 0.2)))#this is for bilstm\n",
        "\n",
        "#model.add(LSTM(lstm_out, recurrent_dropout= 0.2, dropout= 0.2)) #this is for lstm\n",
        "\n",
        "# model.add(Dropout(0.2))\n",
        "#model.add(Dense(5,activation='softmax'))\n",
        "model.add(Dense(2,activation='softmax')) #for sst2\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 46, 300)           5126100   \n",
            "_________________________________________________________________\n",
            "masking_1 (Masking)          (None, 46, 300)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 600)               1442400   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 1202      \n",
            "=================================================================\n",
            "Total params: 6,569,702\n",
            "Trainable params: 6,569,702\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pqsjoTOctTn",
        "outputId": "db058b4b-803a-40b1-e4a8-3aaa92e21284"
      },
      "source": [
        " print(X.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8544, 46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrTVNlgidHEN",
        "outputId": "710beb06-faf4-4db3-e46e-2667fbbec4de"
      },
      "source": [
        "ohe = OneHotEncoder()\n",
        "#Y = ohe.fit_transform(np.array(y_train_ind).reshape(-1,1)).toarray()\n",
        "#y = ohe.fit_transform(np.array(y_test_ind).reshape(-1,1)).toarray()\n",
        "Y = ohe.fit_transform(np.array(y_sst2_t).reshape(-1,1)).toarray()\n",
        "y = ohe.fit_transform(np.array(y_sst2_tes).reshape(-1,1)).toarray()\n",
        "model.fit(X,Y, batch_size =batch_size, epochs = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "267/267 [==============================] - 278s 1s/step - loss: 0.6510 - accuracy: 0.6088\n",
            "Epoch 2/5\n",
            "267/267 [==============================] - 270s 1s/step - loss: 0.4328 - accuracy: 0.8084\n",
            "Epoch 3/5\n",
            "267/267 [==============================] - 270s 1s/step - loss: 0.3289 - accuracy: 0.8623\n",
            "Epoch 4/5\n",
            "267/267 [==============================] - 271s 1s/step - loss: 0.2498 - accuracy: 0.8940\n",
            "Epoch 5/5\n",
            "267/267 [==============================] - 271s 1s/step - loss: 0.1746 - accuracy: 0.9315\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9afc5ea910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIG2ZRTndJH9",
        "outputId": "bff16c01-e276-43c4-e8cf-66bc32c13f7a"
      },
      "source": [
        "score,evalu=model.evaluate(x,y,verbose=2,batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70/70 - 14s - loss: 0.6627 - accuracy: 0.7425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Q2EvV-XdLLe",
        "outputId": "71b46ccd-9b8d-43f5-f4a1-5eeeb888c944"
      },
      "source": [
        "print(evalu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7425339221954346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S91WPzisFZ3i"
      },
      "source": [
        "LSTM/BiLSTM for glove embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vub3ckN6alvm",
        "outputId": "24e6a02d-bbe2-4828-a6bb-08e3975a3c29"
      },
      "source": [
        "#this is for glove embedded sentences using lstm and bi-lstm\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding,Bidirectional\n",
        "from keras.layers import Dense\n",
        "lstm_out = 300\n",
        "batch_size = 32\n",
        "model = Sequential()\n",
        " \n",
        "model.add(Embedding(len(vocabulary),300,trainable=False,weights=[embedding_matrix]))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Masking(mask_value=0))\n",
        "model.add(LSTM(lstm_out, recurrent_dropout = 0.2, dropout = 0.2))\n",
        "#model.add(Bidirectional(LSTM(lstm_out, recurrent_dropout = 0.2, dropout = 0.2)))#this is for bi-lstm\n",
        "model.add(Dense(5,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, None, 300)         5126100   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, None, 300)         0         \n",
            "_________________________________________________________________\n",
            "masking_5 (Masking)          (None, None, 300)         0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 300)               721200    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 1505      \n",
            "=================================================================\n",
            "Total params: 5,848,805\n",
            "Trainable params: 722,705\n",
            "Non-trainable params: 5,126,100\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjv8KMbuF_IH"
      },
      "source": [
        "BERT_EMBEDDINGS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sryMo_VYF91_",
        "outputId": "0b81bb40-f099-4c8e-f6d3-604dc7b433af"
      },
      "source": [
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.0.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INrzRMFPOmPn"
      },
      "source": [
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "#sentences = ['This framework generates embeddings for each input sentence',  'Sentences are passed as a list of string.', 'The quick brown fox jumps over the lazy dog.']\n",
        "# sentence_embeddings = model.encode(x_train_sent['sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8ATx-W5O25G"
      },
      "source": [
        "\n",
        "sentence_embeddings_test=model.encode(x_test_sent['sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH_OFTNoPC1w"
      },
      "source": [
        "sentence_embeddings=model.encode(x_train_sent['sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E-VmGQoKE-r"
      },
      "source": [
        "#already trained bert embeddings\n",
        "import pickle\n",
        "sentence_embeddings=pickle.load(open('/content/drive/MyDrive/embeding.pickle', 'rb'))\n",
        "len(sentence_embeddings)\n",
        "sentence_embeddings_test=pickle.load(open('/content/drive/MyDrive/embeding_test.pickle','rb'))\n",
        "#print(sentence_embeddings_test[0].size)\n",
        "#print(x_train_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKcmnZAyGGEf"
      },
      "source": [
        "Neural Network_model for bert\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhbRUhHhOnmo",
        "outputId": "8fb1b342-f818-48f5-fd5e-7628a344d955"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "# sc = StandardScaler()\n",
        "# X = sc.fit_transform(X)\n",
        "ohe = OneHotEncoder()\n",
        "\n",
        "y_test_ind = ohe.fit_transform(np.array(y_test_ind).reshape(-1,1)).toarray()\n",
        "y_train_ind = ohe.fit_transform(np.array(y_train_ind).reshape(-1,1)).toarray()\n",
        "# X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.1)\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=(768), activation='relu'))\n",
        "# model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U72PiV7cRB23",
        "outputId": "fed666cd-7b8c-4fb4-c4ce-57bfaa9a0a29"
      },
      "source": [
        "history = model.fit(np.array(sentence_embeddings), y_train_ind, epochs=1, batch_size=32)\n",
        "y_pred = model.predict(np.array(sentence_embeddings_test))\n",
        "y_pred=np.array(y_pred)\n",
        "pred = list()\n",
        "for i in range(len(y_pred)):\n",
        "    pred.append(np.argmax(y_pred[i]))\n",
        "test = list()\n",
        "for i in range(len(y_test_ind)):\n",
        "    test.append(np.argmax(y_test_ind[i]))\n",
        "a = accuracy_score(pred,test)\n",
        "print('Accuracy is:', a*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "267/267 [==============================] - 14s 3ms/step - loss: 1.4396 - accuracy: 0.3718\n",
            "Accuracy is: 45.294117647058826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTPtJXPbPUMr",
        "outputId": "93a1f3f6-de7f-4d99-d768-8a47bdc9a142"
      },
      "source": [
        "y_train_ind.size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42720"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZLClgwSGSIq"
      },
      "source": [
        "different ml classifiers trying on bert embeddings\n",
        "->\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-bVdSrJKdX8",
        "outputId": "97669391-c987-4a82-ae28-965d0fc51c8d"
      },
      "source": [
        "#lr.fit(np.array(sentence_embeddings),np.array(y_train_ind))\n",
        "#y_pred=lr.predict(np.array(sentence_embeddings_test))\n",
        "#lr.fit(np.array(sentence_embeddings),np.array(y_sst2_t))\n",
        "#y_pred=lr.predict(np.array(sentence_embeddings_test))\n",
        "\n",
        "\n",
        "\n",
        "# knn.fit(sentence_embeddings,y_train_ind)\n",
        "#knn.fit(sentence_embeddings,y_sst2_t)\n",
        "#y_pred=knn.predict(np.array(sentence_embeddings_test))\n",
        "\n",
        "\n",
        "# gb_1.fit(sentence_embeddings,y_train_ind)\n",
        "#gb_1.fit(sentence_embeddings,y_sst2_t)\n",
        "#y_pred=gb_1.predict(np.array(sentence_embeddings_test))\n",
        "\n",
        "# gnb_1.fit(sentence_embeddings,y_train_ind)\n",
        "#gnb_1.fit(np.array(sentence_embeddings),np.array(y_sst2_t))\n",
        "#y_pred=gnb_1.predict(np.array(sentence_embeddings_test))\n",
        "\n",
        "\n",
        "\n",
        "rf.fit(np.array(sentence_embeddings),np.array(y_train_ind))\n",
        "#rf.fit(np.array(sentence_embeddings),np.array(y_sst2_t))\n",
        "y_pred=rf.predict(np.array(sentence_embeddings_test))\n",
        "\n",
        "#smx.fit(sentence_embeddings,y_train_ind)\n",
        "#smx.fit(sentence_embeddings,y_sst2_t)\n",
        "#y_pred=smx.predict(np.array(sentence_embeddings_test))\n",
        "\n",
        "#xgb_1.fit(np.array(sentence_embeddings),y_train_ind)\n",
        "#xgb_1.fit(np.array(sentence_embeddings),np.array(y_sst2_t))\n",
        "#y_pred=xgb_1.predict(np.array(sentence_embeddings_test))\n",
        "\n",
        " \n",
        "\n",
        "# l=[(r,z) for r,z in zip(y_test_ind,y_pred)]\n",
        "# print(tuple(x)).[0:10]\n",
        "# print(l)\n",
        "accuracy_score(y_test_ind,y_pred)\n",
        "#accuracy_score(y_sst2_tes,y_pred)\n",
        "# print(y_train_ind)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.467420814479638"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_8ZNeOqHmgl"
      },
      "source": [
        "TEXTBLOB\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HCybE_iHk3G",
        "outputId": "c488333f-92c5-4ab3-a76a-082eaf9c798e"
      },
      "source": [
        "from textblob import TextBlob\n",
        "y_pr=[]\n",
        "for sentence in list(x_test_sent['sentence']):\n",
        "  y_pr.append(TextBlob(sentence).sentiment.polarity)\n",
        "y_pr\n",
        "df=pd.DataFrame(y_pr,columns=['sent'])\n",
        "#df['sent']=pd.cut(df['sent'],bins=5,labels=[0,1,2,3,4])\n",
        "df['sent']=pd.cut(df['sent'],bins=2,labels=[0,1]) #this is for sst2 data set\n",
        "accuracy_score(x_test['label'],df['sent'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.20452488687782805"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMNkkzT7HqSV"
      },
      "source": [
        "Vader Lexicon\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myQJCuNUHsZ_",
        "outputId": "b9744ea5-3c12-455e-e0cc-db928be2e57d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "vader=SentimentIntensityAnalyzer()\n",
        "y_pr=[]\n",
        "for sentence in list(x_test_sent['sentence']):\n",
        "  y_pr.append(vader.polarity_scores(sentence)['compound'])\n",
        "y_pr\n",
        "df=pd.DataFrame(y_pr,columns=['sent'])\n",
        "df['sent']=pd.cut(df['sent'],bins=5,labels=[0,1,2,3,4])\n",
        "#df['sent']=pd.cut(df['sent'],bins=2,labels=[0,1]) #this is for sst2 data set\n",
        "accuracy_score(x_test['label'],df['sent'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3153846153846154"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpWlrpwHH0Oo"
      },
      "source": [
        "\n",
        "Converting SST-5 to SST-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLFxQyENH0_D"
      },
      "source": [
        "def convert(n):\n",
        "  if(n>2):\n",
        "    n=1\n",
        "  else:\n",
        "    n=0\n",
        "  return n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwpgJKK4H2qD",
        "outputId": "fbfc0203-f424-4320-f50c-15b7b26c07f2"
      },
      "source": [
        "y_train_ind=np.array(y_train_ind)\n",
        "y_test_ind=np.array(y_test_ind)\n",
        "y_sst2_t=[]\n",
        "y_sst2_tes=[]\n",
        "for i in y_test_ind:\n",
        "  y_sst2_tes.append(convert(i))\n",
        "for i in y_train_ind:\n",
        "  y_sst2_t.append(convert(i))\n",
        "y_sst2_t=np.array(y_sst2_t)\n",
        "y_sst2_tes=np.array(y_sst2_tes)\n",
        "y_sst2_t\n",
        "y_sst2_t.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8544,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O9UcK12GPCT"
      },
      "source": [
        "PY-Torch sentence embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr3ZdyV5GS9g"
      },
      "source": [
        "# !pip install pytorch-pretrained-bert\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "#logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "text = \"Here is the sentence I want embeddings for.\"\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Print out the tokens.\n",
        "print (tokenized_text)\n",
        "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
        "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
        "       \"fishing on the Mississippi river bank.\"\n",
        "\n",
        "# Add the special tokens.\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Split the sentence into tokens.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Map the token strings to their vocabulary indeces.\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# Display the words with their indeces.\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n",
        "    # Mark each of the 22 tokens as belonging to sentence \"1\".\n",
        "segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "print (segments_ids)\n",
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
        "print (\"Number of layers:\", len(encoded_layers))\n",
        "layer_i = 0\n",
        "\n",
        "print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
        "batch_i = 0\n",
        "\n",
        "print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
        "token_i = 0\n",
        "\n",
        "print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))\n",
        "\n",
        "# Concatenate the tensors for all layers. We use `stack` here to\n",
        "# create a new dimension in the tensor.\n",
        "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
        "\n",
        "token_embeddings.size()\n",
        "# Remove dimension 1, the \"batches\".\n",
        "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "\n",
        "token_embeddings.size()\n",
        "# Swap dimensions 0 and 1.\n",
        "token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "token_embeddings.size()\n",
        "# Stores the token vectors, with shape [22 x 3,072]\n",
        "token_vecs_cat = []\n",
        "\n",
        "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
        "\n",
        "# For each token in the sentence...\n",
        "for token in token_embeddings:\n",
        "    \n",
        "    # `token` is a [12 x 768] tensor\n",
        "\n",
        "    # Concatenate the vectors (that is, append them together) from the last \n",
        "    # four layers.\n",
        "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
        "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
        "    \n",
        "    # Use `cat_vec` to represent `token`.\n",
        "    token_vecs_cat.append(cat_vec)\n",
        "\n",
        "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNMf6H4uGUHX"
      },
      "source": [
        "import torch\n",
        "torch.Size([12, 1, 22, 768])\n",
        "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "\n",
        "token_embeddings.size()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}